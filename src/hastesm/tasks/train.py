import sqlite3
from logging import Logger
from pathlib import Path
from typing import List

from ..generate_slurm_scripts import generate_train_script
from ..hastesm_types import HastesmParams
from ..utils import run_command, submit_slurm_job, wait_for_job


def prepare_training_data(name: str, db: Path, search_db: Path, output_dir: Path) -> Path:
	"""Prepare input file for chemprop training

	Args:
	----
		name: Name of the job
		db: Path to the HASTESM SMILES database
		search_db: Path to the search database generated by import_search
		output_dir: Path to the output directory

	Raises:
	------
		ValueError: If the search_db does not exist

	Returns:
	-------
		Path to the training data CSV file

	"""
	if not search_db.exists():
		raise ValueError(f'{search_db} does not exist')

	conn = sqlite3.connect(db)
	c = conn.cursor()
	c.execute(f'ATTACH DATABASE "{search_db}" AS d')
	conn.commit()

	query = """
    SELECT smiles, data.hastenid, searching_data.similarity
    FROM data
    JOIN searching_data ON data.hastenid = searching_data.hastenid
    WHERE similarity IS NOT NULL
    ORDER BY searching_data.similarity DESC
    """

	data_path = output_dir / f'train_{name}.csv'

	with open(data_path, 'wt') as f:
		f.write('smiles,hastenid,similarity\n')
		for smiles, hastenid, similarity in c.execute(query):
			f.write(f'{smiles},{hastenid},{similarity}\n')

	conn.close()

	return data_path


def prepare_pred_scripts(
	name: str,
	output_dir: Path,
	db: Path,
	pred_cpu: int,
	chunk_size: int,
	model_path: Path,
	pred_cutoff: float,
	init_conda: str,
	activate_conda: str,
	logger: Logger,
) -> None:
	"""Prepare chunked chemprop prediction scripts for each prediction CPU

	Args:
	----
		name: Name of the job
		output_dir: Path to the output directory
		db: Path to the HASTESM SMILES database
		pred_cpu: Number of prediction CPUs
		chunk_size:  Size of the prediction chunks
		model_path: Path to the trained chemprop model
		pred_cutoff: Cutoff value for the prediction
		init_conda: Command to initialize conda
		activate_conda: Command to activate the conda environment
		logger: logging.Logger instance

	"""
	conn = sqlite3.connect(db)
	c = conn.cursor()
	number_of_mols = c.execute('SELECT MAX(_ROWID_) FROM data LIMIT 1').fetchone()[0]

	base_size = number_of_mols // pred_cpu
	remainder = number_of_mols % pred_cpu

	logger.info(f'Number of molecules in the database: {number_of_mols}')
	logger.info(f'Molecules per CPU: {base_size}')

	mols_per_cpu = [base_size + 1 if i < remainder else base_size for i in range(pred_cpu)]

	start = 0
	chunk_counter = 0
	for cpu_id, cpu_chunk in enumerate(mols_per_cpu, start=1):
		end = start + cpu_chunk

		script_path = output_dir / f'pred_{name}_cpu{cpu_id}.sh'
		pred_dir = f'PRED{cpu_id}'

		with open(script_path, 'w') as f:
			f.write('#!/bin/bash\n')
			f.write(init_conda + '\n')
			f.write(activate_conda + '\n')

			for chunk_start in range(start, end, chunk_size):
				chunk_end = min(chunk_start + chunk_size, end)
				query = f'SELECT smiles, hastenid FROM data WHERE hastenid >= {chunk_start} AND hastenid < {chunk_end}'

				inputfile = f'{pred_dir}/input_pred_chunk{chunk_counter}.csv'
				tmpfile = f'{pred_dir}/tmp_pred_chunk{chunk_counter}.csv'
				outputfile = f'{pred_dir}/output_pred_chunk{chunk_counter}.csv'

				f.write(f'mkdir -p {pred_dir}\n')
				f.write(f'echo smiles,hastenid > {inputfile}\n')
				f.write(f'sqlite3 -separator "," {db} "{query}" >> {inputfile}\n')
				f.write(
					'OMP_NUM_THREADS=1 nice chemprop predict --num-workers 0'
					' --accelerator cpu --devices 1 --target-columns similarity'
					f' --test-path {inputfile} --model-path {model_path} --preds-path {tmpfile}\n'
				)
				f.write(f'awk -F, \'NR==1 || $3 >= {pred_cutoff} {{ print $2","$3 }}\' {tmpfile} > {outputfile}\n')
				f.write(f'rm {inputfile} {tmpfile}\n')

				chunk_counter += 1

		start = end


def train(params: HastesmParams, logger: Logger, job_ids: List[str]) -> None:
	"""Trains the chemprop model and creates the prediction scripts

	Args:
	----
		params: Parsed HastesmParams from the main file
		logger: logging.Logger instance
		job_ids: List of slurm job ids where the train job id is added

	"""
	training_data_path = prepare_training_data(params.name, params.db, params.search_db, params.output_dir)

	logger.info(f'Wrote training data to: {training_data_path}')

	run_command(['pigz', '-f', training_data_path], 'Error compressing training data', logger)

	script_path = generate_train_script(
		params.name,
		params.output_dir,
		params.local_dir,
		params.model_path,
		params.gpu_partition,
		params.init_conda,
		params.activate_conda,
		params.debug,
	)

	train_jobid = submit_slurm_job(script_path, logger)
	job_ids.append(train_jobid)

	logger.info(f'Submitted chemprop training job with ID {train_jobid}')

	# Prepare prediction scripts while the model is training
	prepare_pred_scripts(
		params.name,
		params.output_dir,
		params.db,
		params.pred_cpu,
		params.predchunksize,
		params.model_path,
		params.pred_cutoff,
		params.init_conda,
		params.activate_conda,
		logger,
	)

	wait_for_job(train_jobid, job_ids)

	logger.info(f'Training job {train_jobid} completed')
